@inproceedings{ceolini_combining_2019,
 abstract = {This work presents a multi-channel speech enhancement algorithm using a neural network combined with beamforming deployed realtime on a wireless acoustic sensor network (WASN) of distributed microphones. We combine spectral mask estimation via a deep neural network together with spatial ﬁltering to obtain a robust speech enhancement system even in difﬁcult real-world scenarios (e.g. speech in noise, reverberant environments). Although the model is trained on simulated data, it performs comparably well on realworld tasks relative to an ideal oracle beamformer. We show that the model can be deployed on a WASN platform that allows for remote placement of microphones and on-board computing. We consider models with a small parameter count and low computational complexity. It achieves signal-to-distortion ratio (SDR) improvements of up to 10 dB in a real-world scenario and runs real-time on-board the WASN, with a latency in the order of hundreds of milliseconds.},
 author = {Ceolini, Enea and Liu, Shih-Chii},
 booktitle = {2019 {IEEE} 29th {International} {Workshop} on {Machine} {Learning} for {Signal} {Processing} ({MLSP})},
 doi = {10.1109/MLSP.2019.8918787},
 file = {Ceolini and Liu - 2019 - Combining Deep Neural Networks and Beamforming for.pdf:C\:\\Users\\pdidier\\Zotero\\storage\\Z3IZCACQ\\Ceolini and Liu - 2019 - Combining Deep Neural Networks and Beamforming for.pdf:application/pdf},
 language = {en},
 month = {October},
 pages = {1--6},
 publisher = {IEEE},
 title = {Combining {Deep} {Neural} {Networks} and {Beamforming} for {Real}-{Time} {Multi}-{Channel} {Speech} {Enhancement} using a {Wireless} {Acoustic} {Sensor} {Network}},
 url = {https://ieeexplore.ieee.org/document/8918787/},
 urldate = {2022-08-21},
 year = {2019}
}

@inproceedings{heymann_neural_2016,
 abstract = {We present a neural network based approach to acoustic beamforming. The network is used to estimate spectral masks from which the Cross-Power Spectral Density matrices of speech and noise are estimated, which in turn are used to compute the beamformer coefﬁcients. The network training is independent of the number and the geometric conﬁguration of the microphones. We further show that it is possible to train the network on clean speech only, avoiding the need for stereo data with separated speech and noise. Two types of networks are evaluated. One small feed-forward network with only one hidden layer and one more elaborated bi-directional Long Short-Term Memory network. We compare our system with different parametric approaches to mask estimation and using different beamforming algorithms. We show that our system yields superior results, both in terms of perceptual speech quality and with respect to speech recognition error rate. The results for the simple feed-forward network are especially encouraging considering its low computational requirements.},
 author = {Heymann, Jahn and Drude, Lukas and Haeb-Umbach, Reinhold},
 booktitle = {2016 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
 doi = {10.1109/ICASSP.2016.7471664},
 file = {Heymann et al. - 2016 - Neural network based spectral mask estimation for .pdf:C\:\\Users\\pdidier\\Zotero\\storage\\ULYSP2PM\\Heymann et al. - 2016 - Neural network based spectral mask estimation for .pdf:application/pdf},
 language = {en},
 month = {March},
 pages = {196--200},
 publisher = {IEEE},
 title = {Neural network based spectral mask estimation for acoustic beamforming},
 url = {http://ieeexplore.ieee.org/document/7471664/},
 urldate = {2022-08-21},
 year = {2016}
}
